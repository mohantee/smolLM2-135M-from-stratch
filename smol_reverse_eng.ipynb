{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrates the approach to download the SmolLM2 model of 135M parameters and reverse Engineer to create the model from scratch and train for a custom data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "u_the1rubPsv"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "#   SmolLM2-135M â€” Reverse Engineering / Architecture Dump\n",
        "#   Goal: Inspect config, tokenizer, weights, and derive\n",
        "#         all architecture details for re-implementation.\n",
        "# ============================================================\n",
        "\n",
        "!pip install transformers safetensors huggingface-hub accelerate -q\n",
        "import json, torch\n",
        "from huggingface_hub import hf_hub_download\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from safetensors.torch import load_file\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFhpx6xnjXW1",
        "outputId": "eb5d6712-9ec7-4dcc-9cfd-1d4ee002ebf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== Downloading model files ====\n",
            "Paths:\n",
            "config.json: /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/config.json\n",
            "tokenizer.json: /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/tokenizer.json\n",
            "model.safetensors: /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/model.safetensors\n"
          ]
        }
      ],
      "source": [
        "REPO = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "\n",
        "print(\"==== Downloading model files ====\")\n",
        "cfg_path = hf_hub_download(REPO, \"config.json\")\n",
        "tok_path = hf_hub_download(REPO, \"tokenizer.json\")\n",
        "weights_path = hf_hub_download(REPO, \"model.safetensors\")\n",
        "\n",
        "print(\"Paths:\")\n",
        "print(\"config.json:\", cfg_path)\n",
        "print(\"tokenizer.json:\", tok_path)\n",
        "print(\"model.safetensors:\", weights_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDe9F9u0jZ2t",
        "outputId": "7e0e41fa-3aca-4f29-e8c7-86b084926f91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================== CONFIG.JSON ========================\n",
            "{'architectures': ['LlamaForCausalLM'],\n",
            " 'attention_bias': False,\n",
            " 'attention_dropout': 0.0,\n",
            " 'bos_token_id': 0,\n",
            " 'eos_token_id': 0,\n",
            " 'hidden_act': 'silu',\n",
            " 'hidden_size': 576,\n",
            " 'initializer_range': 0.041666666666666664,\n",
            " 'intermediate_size': 1536,\n",
            " 'is_llama_config': True,\n",
            " 'max_position_embeddings': 8192,\n",
            " 'model_type': 'llama',\n",
            " 'num_attention_heads': 9,\n",
            " 'num_hidden_layers': 30,\n",
            " 'num_key_value_heads': 3,\n",
            " 'pretraining_tp': 1,\n",
            " 'rms_norm_eps': 1e-05,\n",
            " 'rope_interleaved': False,\n",
            " 'rope_scaling': None,\n",
            " 'rope_theta': 100000,\n",
            " 'tie_word_embeddings': True,\n",
            " 'torch_dtype': 'bfloat16',\n",
            " 'transformers_version': '4.40.1',\n",
            " 'use_cache': True,\n",
            " 'vocab_size': 49152}\n"
          ]
        }
      ],
      "source": [
        "# 1. Load CONFIG\n",
        "print(\"\\n======================== CONFIG.JSON ========================\")\n",
        "config = json.load(open(cfg_path))\n",
        "pprint(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EydbXV_gjhg0",
        "outputId": "81e7cb21-0d2e-465c-a137-fa7dced9b805"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Extracted Architecture Summary ===\n",
            "Model dimension (d_model):          576\n",
            "Number of layers:                    30\n",
            "Attention heads:                     9\n",
            "Key/Value heads:                     3\n",
            "Intermediate (FFN hidden):           1536\n",
            "Vocabulary size:                     49152\n",
            "Max positions:                       8192\n",
            "Activation:                           silu\n",
            "RMSNorm eps:                          1e-05\n",
            "RoPE theta:                           100000\n",
            "RoPE scaling:                         None\n",
            "RoPE interleaved:                     False\n",
            "\n",
            "=== Derived Attention Dimensions ===\n",
            "Head dimension:                      64\n",
            "QKV total dim:                       1344 (subject to weight inspection)\n"
          ]
        }
      ],
      "source": [
        "# Extract important config values\n",
        "d_model       = config.get(\"hidden_size\")\n",
        "n_layers      = config.get(\"num_hidden_layers\")\n",
        "n_heads       = config.get(\"num_attention_heads\")\n",
        "n_kv_heads    = config.get(\"num_key_value_heads\")\n",
        "intermediate  = config.get(\"intermediate_size\")\n",
        "vocab_size    = config.get(\"vocab_size\")\n",
        "max_pos       = config.get(\"max_position_embeddings\")\n",
        "rope_theta    = config.get(\"rope_theta\")\n",
        "rope_scaling  = config.get(\"rope_scaling\")\n",
        "rope_inter    = config.get(\"rope_interleaved\")\n",
        "norm_eps      = config.get(\"rms_norm_eps\")\n",
        "activation    = config.get(\"hidden_act\")\n",
        "\n",
        "print(\"\\n=== Extracted Architecture Summary ===\")\n",
        "print(f\"Model dimension (d_model):          {d_model}\")\n",
        "print(f\"Number of layers:                    {n_layers}\")\n",
        "print(f\"Attention heads:                     {n_heads}\")\n",
        "print(f\"Key/Value heads:                     {n_kv_heads}\")\n",
        "print(f\"Intermediate (FFN hidden):           {intermediate}\")\n",
        "print(f\"Vocabulary size:                     {vocab_size}\")\n",
        "print(f\"Max positions:                       {max_pos}\")\n",
        "print(f\"Activation:                           {activation}\")\n",
        "print(f\"RMSNorm eps:                          {norm_eps}\")\n",
        "print(f\"RoPE theta:                           {rope_theta}\")\n",
        "print(f\"RoPE scaling:                         {rope_scaling}\")\n",
        "print(f\"RoPE interleaved:                     {rope_inter}\")\n",
        "\n",
        "# Derived shapes\n",
        "print(\"\\n=== Derived Attention Dimensions ===\")\n",
        "head_dim = d_model // n_heads\n",
        "print(f\"Head dimension:                      {head_dim}\")\n",
        "print(f\"QKV total dim:                       {d_model + d_model + (head_dim * n_kv_heads)} (subject to weight inspection)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObuhlFjvjnjE",
        "outputId": "a0c9dc17-8f2a-4b33-91ac-3bc94a42049a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================== TOKENIZER ==========================\n",
            "Tokenizer vocab size: 49152\n",
            "Tokenizer special tokens: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<|endoftext|>', '<|im_start|>', '<|im_end|>', '<repo_name>', '<reponame>', '<file_sep>', '<filename>', '<gh_stars>', '<issue_start>', '<issue_comment>', '<issue_closed>', '<jupyter_start>', '<jupyter_text>', '<jupyter_code>', '<jupyter_output>', '<jupyter_script>', '<empty_output>']}\n"
          ]
        }
      ],
      "source": [
        "# 2. Load TOKENIZER\n",
        "print(\"\\n======================== TOKENIZER ==========================\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(REPO)\n",
        "print(\"Tokenizer vocab size:\", len(tokenizer))\n",
        "print(\"Tokenizer special tokens:\", tokenizer.special_tokens_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lli0PDJmlVZc",
        "outputId": "f9869010-bf98-4f59-c841-91164d3a40e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================== WEIGHT KEYS ========================\n",
            "Total tensors: 272\n",
            "\n",
            "First 300 keys:\n",
            "model.embed_tokens.weight torch.Size([49152, 576])\n",
            "model.layers.0.input_layernorm.weight torch.Size([576])\n",
            "model.layers.0.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.0.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.0.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.0.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.0.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.0.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.0.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.0.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.1.input_layernorm.weight torch.Size([576])\n",
            "model.layers.1.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.1.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.1.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.1.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.1.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.1.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.1.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.1.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.10.input_layernorm.weight torch.Size([576])\n",
            "model.layers.10.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.10.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.10.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.10.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.10.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.10.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.10.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.10.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.11.input_layernorm.weight torch.Size([576])\n",
            "model.layers.11.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.11.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.11.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.11.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.11.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.11.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.11.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.11.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.12.input_layernorm.weight torch.Size([576])\n",
            "model.layers.12.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.12.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.12.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.12.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.12.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.12.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.12.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.12.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.13.input_layernorm.weight torch.Size([576])\n",
            "model.layers.13.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.13.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.13.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.13.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.13.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.13.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.13.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.13.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.14.input_layernorm.weight torch.Size([576])\n",
            "model.layers.14.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.14.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.14.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.14.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.14.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.14.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.14.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.14.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.15.input_layernorm.weight torch.Size([576])\n",
            "model.layers.15.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.15.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.15.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.15.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.15.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.15.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.15.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.15.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.16.input_layernorm.weight torch.Size([576])\n",
            "model.layers.16.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.16.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.16.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.16.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.16.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.16.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.16.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.16.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.17.input_layernorm.weight torch.Size([576])\n",
            "model.layers.17.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.17.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.17.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.17.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.17.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.17.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.17.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.17.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.18.input_layernorm.weight torch.Size([576])\n",
            "model.layers.18.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.18.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.18.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.18.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.18.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.18.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.18.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.18.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.19.input_layernorm.weight torch.Size([576])\n",
            "model.layers.19.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.19.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.19.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.19.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.19.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.19.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.19.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.19.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.2.input_layernorm.weight torch.Size([576])\n",
            "model.layers.2.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.2.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.2.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.2.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.2.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.2.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.2.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.2.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.20.input_layernorm.weight torch.Size([576])\n",
            "model.layers.20.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.20.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.20.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.20.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.20.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.20.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.20.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.20.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.21.input_layernorm.weight torch.Size([576])\n",
            "model.layers.21.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.21.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.21.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.21.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.21.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.21.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.21.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.21.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.22.input_layernorm.weight torch.Size([576])\n",
            "model.layers.22.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.22.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.22.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.22.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.22.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.22.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.22.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.22.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.23.input_layernorm.weight torch.Size([576])\n",
            "model.layers.23.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.23.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.23.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.23.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.23.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.23.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.23.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.23.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.24.input_layernorm.weight torch.Size([576])\n",
            "model.layers.24.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.24.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.24.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.24.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.24.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.24.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.24.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.24.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.25.input_layernorm.weight torch.Size([576])\n",
            "model.layers.25.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.25.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.25.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.25.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.25.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.25.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.25.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.25.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.26.input_layernorm.weight torch.Size([576])\n",
            "model.layers.26.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.26.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.26.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.26.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.26.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.26.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.26.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.26.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.27.input_layernorm.weight torch.Size([576])\n",
            "model.layers.27.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.27.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.27.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.27.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.27.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.27.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.27.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.27.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.28.input_layernorm.weight torch.Size([576])\n",
            "model.layers.28.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.28.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.28.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.28.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.28.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.28.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.28.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.28.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.29.input_layernorm.weight torch.Size([576])\n",
            "model.layers.29.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.29.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.29.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.29.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.29.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.29.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.29.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.29.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.3.input_layernorm.weight torch.Size([576])\n",
            "model.layers.3.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.3.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.3.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.3.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.3.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.3.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.3.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.3.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.4.input_layernorm.weight torch.Size([576])\n",
            "model.layers.4.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.4.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.4.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.4.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.4.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.4.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.4.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.4.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.5.input_layernorm.weight torch.Size([576])\n",
            "model.layers.5.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.5.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.5.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.5.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.5.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.5.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.5.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.5.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.6.input_layernorm.weight torch.Size([576])\n",
            "model.layers.6.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.6.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.6.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.6.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.6.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.6.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.6.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.6.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.7.input_layernorm.weight torch.Size([576])\n",
            "model.layers.7.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.7.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.7.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.7.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.7.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.7.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.7.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.7.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.8.input_layernorm.weight torch.Size([576])\n",
            "model.layers.8.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.8.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.8.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.8.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.8.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.8.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.8.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.8.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.layers.9.input_layernorm.weight torch.Size([576])\n",
            "model.layers.9.mlp.down_proj.weight torch.Size([576, 1536])\n",
            "model.layers.9.mlp.gate_proj.weight torch.Size([1536, 576])\n",
            "model.layers.9.mlp.up_proj.weight torch.Size([1536, 576])\n",
            "model.layers.9.post_attention_layernorm.weight torch.Size([576])\n",
            "model.layers.9.self_attn.k_proj.weight torch.Size([192, 576])\n",
            "model.layers.9.self_attn.o_proj.weight torch.Size([576, 576])\n",
            "model.layers.9.self_attn.q_proj.weight torch.Size([576, 576])\n",
            "model.layers.9.self_attn.v_proj.weight torch.Size([192, 576])\n",
            "model.norm.weight torch.Size([576])\n",
            "\n",
            "======================== LAYER GROUPING ========================\n",
            "Detected layer groups: 30\n",
            "Numeric layer count: 30\n",
            "Sample layer IDs: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']  ...\n",
            "\n",
            "======================== LAYER STRUCTURE DUMP ========================\n",
            "\n",
            "----- Layer 0 -----\n",
            "model.layers.0.input_layernorm.weight -> torch.Size([576]) torch.bfloat16\n",
            "model.layers.0.mlp.down_proj.weight -> torch.Size([576, 1536]) torch.bfloat16\n",
            "model.layers.0.mlp.gate_proj.weight -> torch.Size([1536, 576]) torch.bfloat16\n",
            "model.layers.0.mlp.up_proj.weight -> torch.Size([1536, 576]) torch.bfloat16\n",
            "model.layers.0.post_attention_layernorm.weight -> torch.Size([576]) torch.bfloat16\n",
            "model.layers.0.self_attn.k_proj.weight -> torch.Size([192, 576]) torch.bfloat16\n",
            "model.layers.0.self_attn.o_proj.weight -> torch.Size([576, 576]) torch.bfloat16\n",
            "model.layers.0.self_attn.q_proj.weight -> torch.Size([576, 576]) torch.bfloat16\n",
            "model.layers.0.self_attn.v_proj.weight -> torch.Size([192, 576]) torch.bfloat16\n",
            "\n",
            "----- Layer 1 -----\n",
            "model.layers.1.input_layernorm.weight -> torch.Size([576]) torch.bfloat16\n",
            "model.layers.1.mlp.down_proj.weight -> torch.Size([576, 1536]) torch.bfloat16\n",
            "model.layers.1.mlp.gate_proj.weight -> torch.Size([1536, 576]) torch.bfloat16\n",
            "model.layers.1.mlp.up_proj.weight -> torch.Size([1536, 576]) torch.bfloat16\n",
            "model.layers.1.post_attention_layernorm.weight -> torch.Size([576]) torch.bfloat16\n",
            "model.layers.1.self_attn.k_proj.weight -> torch.Size([192, 576]) torch.bfloat16\n",
            "model.layers.1.self_attn.o_proj.weight -> torch.Size([576, 576]) torch.bfloat16\n",
            "model.layers.1.self_attn.q_proj.weight -> torch.Size([576, 576]) torch.bfloat16\n",
            "model.layers.1.self_attn.v_proj.weight -> torch.Size([192, 576]) torch.bfloat16\n",
            "\n",
            "----- Layer 2 -----\n",
            "model.layers.2.input_layernorm.weight -> torch.Size([576]) torch.bfloat16\n",
            "model.layers.2.mlp.down_proj.weight -> torch.Size([576, 1536]) torch.bfloat16\n",
            "model.layers.2.mlp.gate_proj.weight -> torch.Size([1536, 576]) torch.bfloat16\n",
            "model.layers.2.mlp.up_proj.weight -> torch.Size([1536, 576]) torch.bfloat16\n",
            "model.layers.2.post_attention_layernorm.weight -> torch.Size([576]) torch.bfloat16\n",
            "model.layers.2.self_attn.k_proj.weight -> torch.Size([192, 576]) torch.bfloat16\n",
            "model.layers.2.self_attn.o_proj.weight -> torch.Size([576, 576]) torch.bfloat16\n",
            "model.layers.2.self_attn.q_proj.weight -> torch.Size([576, 576]) torch.bfloat16\n",
            "model.layers.2.self_attn.v_proj.weight -> torch.Size([192, 576]) torch.bfloat16\n",
            "\n",
            "----- Layer 3 -----\n",
            "model.layers.3.input_layernorm.weight -> torch.Size([576]) torch.bfloat16\n",
            "model.layers.3.mlp.down_proj.weight -> torch.Size([576, 1536]) torch.bfloat16\n",
            "model.layers.3.mlp.gate_proj.weight -> torch.Size([1536, 576]) torch.bfloat16\n",
            "model.layers.3.mlp.up_proj.weight -> torch.Size([1536, 576]) torch.bfloat16\n",
            "model.layers.3.post_attention_layernorm.weight -> torch.Size([576]) torch.bfloat16\n",
            "model.layers.3.self_attn.k_proj.weight -> torch.Size([192, 576]) torch.bfloat16\n",
            "model.layers.3.self_attn.o_proj.weight -> torch.Size([576, 576]) torch.bfloat16\n",
            "model.layers.3.self_attn.q_proj.weight -> torch.Size([576, 576]) torch.bfloat16\n",
            "model.layers.3.self_attn.v_proj.weight -> torch.Size([192, 576]) torch.bfloat16\n",
            "\n",
            "----- Layer 4 -----\n",
            "model.layers.4.input_layernorm.weight -> torch.Size([576]) torch.bfloat16\n",
            "model.layers.4.mlp.down_proj.weight -> torch.Size([576, 1536]) torch.bfloat16\n",
            "model.layers.4.mlp.gate_proj.weight -> torch.Size([1536, 576]) torch.bfloat16\n",
            "model.layers.4.mlp.up_proj.weight -> torch.Size([1536, 576]) torch.bfloat16\n",
            "model.layers.4.post_attention_layernorm.weight -> torch.Size([576]) torch.bfloat16\n",
            "model.layers.4.self_attn.k_proj.weight -> torch.Size([192, 576]) torch.bfloat16\n",
            "model.layers.4.self_attn.o_proj.weight -> torch.Size([576, 576]) torch.bfloat16\n",
            "model.layers.4.self_attn.q_proj.weight -> torch.Size([576, 576]) torch.bfloat16\n",
            "model.layers.4.self_attn.v_proj.weight -> torch.Size([192, 576]) torch.bfloat16\n",
            "\n",
            "(Full layer dump printed for first 5 numeric layers. Modify slice to print all.)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "# 3. Inspect WEIGHTS (fixed)\n",
        "print(\"\\n======================== WEIGHT KEYS ========================\")\n",
        "state = load_file(weights_path)   # assume this returns your checkpoint dict\n",
        "all_keys = list(state.keys())\n",
        "\n",
        "print(\"Total tensors:\", len(all_keys))\n",
        "print(\"\\nFirst 300 keys:\")\n",
        "for k in all_keys[:300]:\n",
        "    v = state[k]\n",
        "    shape = getattr(v, \"shape\", None)\n",
        "    print(k, shape)\n",
        "\n",
        "# Group weights by layer (robustly find the token 'layers' then next part)\n",
        "print(\"\\n======================== LAYER GROUPING ========================\")\n",
        "layers = defaultdict(list)\n",
        "pat = re.compile(r'layers\\.(\\d+)')  # fallback regex\n",
        "\n",
        "for k in all_keys:\n",
        "    if \"layers.\" in k:\n",
        "        parts = k.split('.')\n",
        "        layer_id = None\n",
        "        # try to find the 'layers' token and take the next element\n",
        "        try:\n",
        "            idx = parts.index('layers')\n",
        "            if idx + 1 < len(parts):\n",
        "                layer_id = parts[idx + 1]\n",
        "        except ValueError:\n",
        "            layer_id = None\n",
        "\n",
        "        # fallback to regex if the above didn't find a numeric id\n",
        "        if (layer_id is None) or (not str(layer_id).isdigit()):\n",
        "            m = pat.search(k)\n",
        "            if m:\n",
        "                layer_id = m.group(1)\n",
        "\n",
        "        # as final fallback, keep a sentinel group\n",
        "        if not layer_id:\n",
        "            layer_id = \"__unknown__\"\n",
        "\n",
        "        layers[str(layer_id)].append(k)\n",
        "\n",
        "print(\"Detected layer groups:\", len(layers))\n",
        "numeric_layer_ids = sorted([lid for lid in layers.keys() if lid.isdigit()], key=lambda x: int(x))\n",
        "non_numeric_ids = sorted([lid for lid in layers.keys() if not lid.isdigit()])\n",
        "print(\"Numeric layer count:\", len(numeric_layer_ids))\n",
        "print(\"Sample layer IDs:\", numeric_layer_ids[:10], \" ...\")\n",
        "if non_numeric_ids:\n",
        "    print(\"Non-numeric groups (sample):\", non_numeric_ids[:10])\n",
        "\n",
        "# Print structure of each layer (numeric layers first)\n",
        "print(\"\\n======================== LAYER STRUCTURE DUMP ========================\")\n",
        "to_show = numeric_layer_ids[:5] if numeric_layer_ids else list(layers.keys())[:5]\n",
        "for lid in to_show:\n",
        "    print(f\"\\n----- Layer {lid} -----\")\n",
        "    for k in sorted(layers[lid]):\n",
        "        v = state.get(k)\n",
        "        shape = getattr(v, \"shape\", None)\n",
        "        dtype = getattr(v, \"dtype\", None)\n",
        "        print(k, \"->\", shape, dtype)\n",
        "print(\"\\n(Full layer dump printed for first 5 numeric layers. Modify slice to print all.)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK6QbOAVl8S-",
        "outputId": "d5b2a37c-0a96-488b-96f6-7b37c606f9ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================== TRANSFORMERS MODEL SUMMARY ========================\n",
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(49152, 576)\n",
            "    (layers): ModuleList(\n",
            "      (0-29): 30 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
            "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
            "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
            "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
            "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
            "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
            ")\n",
            "\n",
            "======================== MODULE TREE ========================\n",
            " : LlamaForCausalLM\n",
            "model : LlamaModel\n",
            "model.embed_tokens : Embedding\n",
            "model.layers : ModuleList\n",
            "model.layers.0 : LlamaDecoderLayer\n",
            "model.layers.0.self_attn : LlamaAttention\n",
            "model.layers.0.self_attn.q_proj : Linear\n",
            "model.layers.0.self_attn.k_proj : Linear\n",
            "model.layers.0.self_attn.v_proj : Linear\n",
            "model.layers.0.self_attn.o_proj : Linear\n",
            "model.layers.0.mlp : LlamaMLP\n",
            "model.layers.0.mlp.gate_proj : Linear\n",
            "model.layers.0.mlp.up_proj : Linear\n",
            "model.layers.0.mlp.down_proj : Linear\n",
            "model.layers.0.mlp.act_fn : SiLUActivation\n",
            "model.layers.0.input_layernorm : LlamaRMSNorm\n",
            "model.layers.0.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.1 : LlamaDecoderLayer\n",
            "model.layers.1.self_attn : LlamaAttention\n",
            "model.layers.1.self_attn.q_proj : Linear\n",
            "model.layers.1.self_attn.k_proj : Linear\n",
            "model.layers.1.self_attn.v_proj : Linear\n",
            "model.layers.1.self_attn.o_proj : Linear\n",
            "model.layers.1.mlp : LlamaMLP\n",
            "model.layers.1.mlp.gate_proj : Linear\n",
            "model.layers.1.mlp.up_proj : Linear\n",
            "model.layers.1.mlp.down_proj : Linear\n",
            "model.layers.1.mlp.act_fn : SiLUActivation\n",
            "model.layers.1.input_layernorm : LlamaRMSNorm\n",
            "model.layers.1.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.2 : LlamaDecoderLayer\n",
            "model.layers.2.self_attn : LlamaAttention\n",
            "model.layers.2.self_attn.q_proj : Linear\n",
            "model.layers.2.self_attn.k_proj : Linear\n",
            "model.layers.2.self_attn.v_proj : Linear\n",
            "model.layers.2.self_attn.o_proj : Linear\n",
            "model.layers.2.mlp : LlamaMLP\n",
            "model.layers.2.mlp.gate_proj : Linear\n",
            "model.layers.2.mlp.up_proj : Linear\n",
            "model.layers.2.mlp.down_proj : Linear\n",
            "model.layers.2.mlp.act_fn : SiLUActivation\n",
            "model.layers.2.input_layernorm : LlamaRMSNorm\n",
            "model.layers.2.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.3 : LlamaDecoderLayer\n",
            "model.layers.3.self_attn : LlamaAttention\n",
            "model.layers.3.self_attn.q_proj : Linear\n",
            "model.layers.3.self_attn.k_proj : Linear\n",
            "model.layers.3.self_attn.v_proj : Linear\n",
            "model.layers.3.self_attn.o_proj : Linear\n",
            "model.layers.3.mlp : LlamaMLP\n",
            "model.layers.3.mlp.gate_proj : Linear\n",
            "model.layers.3.mlp.up_proj : Linear\n",
            "model.layers.3.mlp.down_proj : Linear\n",
            "model.layers.3.mlp.act_fn : SiLUActivation\n",
            "model.layers.3.input_layernorm : LlamaRMSNorm\n",
            "model.layers.3.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.4 : LlamaDecoderLayer\n",
            "model.layers.4.self_attn : LlamaAttention\n",
            "model.layers.4.self_attn.q_proj : Linear\n",
            "model.layers.4.self_attn.k_proj : Linear\n",
            "model.layers.4.self_attn.v_proj : Linear\n",
            "model.layers.4.self_attn.o_proj : Linear\n",
            "model.layers.4.mlp : LlamaMLP\n",
            "model.layers.4.mlp.gate_proj : Linear\n",
            "model.layers.4.mlp.up_proj : Linear\n",
            "model.layers.4.mlp.down_proj : Linear\n",
            "model.layers.4.mlp.act_fn : SiLUActivation\n",
            "model.layers.4.input_layernorm : LlamaRMSNorm\n",
            "model.layers.4.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.5 : LlamaDecoderLayer\n",
            "model.layers.5.self_attn : LlamaAttention\n",
            "model.layers.5.self_attn.q_proj : Linear\n",
            "model.layers.5.self_attn.k_proj : Linear\n",
            "model.layers.5.self_attn.v_proj : Linear\n",
            "model.layers.5.self_attn.o_proj : Linear\n",
            "model.layers.5.mlp : LlamaMLP\n",
            "model.layers.5.mlp.gate_proj : Linear\n",
            "model.layers.5.mlp.up_proj : Linear\n",
            "model.layers.5.mlp.down_proj : Linear\n",
            "model.layers.5.mlp.act_fn : SiLUActivation\n",
            "model.layers.5.input_layernorm : LlamaRMSNorm\n",
            "model.layers.5.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.6 : LlamaDecoderLayer\n",
            "model.layers.6.self_attn : LlamaAttention\n",
            "model.layers.6.self_attn.q_proj : Linear\n",
            "model.layers.6.self_attn.k_proj : Linear\n",
            "model.layers.6.self_attn.v_proj : Linear\n",
            "model.layers.6.self_attn.o_proj : Linear\n",
            "model.layers.6.mlp : LlamaMLP\n",
            "model.layers.6.mlp.gate_proj : Linear\n",
            "model.layers.6.mlp.up_proj : Linear\n",
            "model.layers.6.mlp.down_proj : Linear\n",
            "model.layers.6.mlp.act_fn : SiLUActivation\n",
            "model.layers.6.input_layernorm : LlamaRMSNorm\n",
            "model.layers.6.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.7 : LlamaDecoderLayer\n",
            "model.layers.7.self_attn : LlamaAttention\n",
            "model.layers.7.self_attn.q_proj : Linear\n",
            "model.layers.7.self_attn.k_proj : Linear\n",
            "model.layers.7.self_attn.v_proj : Linear\n",
            "model.layers.7.self_attn.o_proj : Linear\n",
            "model.layers.7.mlp : LlamaMLP\n",
            "model.layers.7.mlp.gate_proj : Linear\n",
            "model.layers.7.mlp.up_proj : Linear\n",
            "model.layers.7.mlp.down_proj : Linear\n",
            "model.layers.7.mlp.act_fn : SiLUActivation\n",
            "model.layers.7.input_layernorm : LlamaRMSNorm\n",
            "model.layers.7.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.8 : LlamaDecoderLayer\n",
            "model.layers.8.self_attn : LlamaAttention\n",
            "model.layers.8.self_attn.q_proj : Linear\n",
            "model.layers.8.self_attn.k_proj : Linear\n",
            "model.layers.8.self_attn.v_proj : Linear\n",
            "model.layers.8.self_attn.o_proj : Linear\n",
            "model.layers.8.mlp : LlamaMLP\n",
            "model.layers.8.mlp.gate_proj : Linear\n",
            "model.layers.8.mlp.up_proj : Linear\n",
            "model.layers.8.mlp.down_proj : Linear\n",
            "model.layers.8.mlp.act_fn : SiLUActivation\n",
            "model.layers.8.input_layernorm : LlamaRMSNorm\n",
            "model.layers.8.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.9 : LlamaDecoderLayer\n",
            "model.layers.9.self_attn : LlamaAttention\n",
            "model.layers.9.self_attn.q_proj : Linear\n",
            "model.layers.9.self_attn.k_proj : Linear\n",
            "model.layers.9.self_attn.v_proj : Linear\n",
            "model.layers.9.self_attn.o_proj : Linear\n",
            "model.layers.9.mlp : LlamaMLP\n",
            "model.layers.9.mlp.gate_proj : Linear\n",
            "model.layers.9.mlp.up_proj : Linear\n",
            "model.layers.9.mlp.down_proj : Linear\n",
            "model.layers.9.mlp.act_fn : SiLUActivation\n",
            "model.layers.9.input_layernorm : LlamaRMSNorm\n",
            "model.layers.9.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.10 : LlamaDecoderLayer\n",
            "model.layers.10.self_attn : LlamaAttention\n",
            "model.layers.10.self_attn.q_proj : Linear\n",
            "model.layers.10.self_attn.k_proj : Linear\n",
            "model.layers.10.self_attn.v_proj : Linear\n",
            "model.layers.10.self_attn.o_proj : Linear\n",
            "model.layers.10.mlp : LlamaMLP\n",
            "model.layers.10.mlp.gate_proj : Linear\n",
            "model.layers.10.mlp.up_proj : Linear\n",
            "model.layers.10.mlp.down_proj : Linear\n",
            "model.layers.10.mlp.act_fn : SiLUActivation\n",
            "model.layers.10.input_layernorm : LlamaRMSNorm\n",
            "model.layers.10.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.11 : LlamaDecoderLayer\n",
            "model.layers.11.self_attn : LlamaAttention\n",
            "model.layers.11.self_attn.q_proj : Linear\n",
            "model.layers.11.self_attn.k_proj : Linear\n",
            "model.layers.11.self_attn.v_proj : Linear\n",
            "model.layers.11.self_attn.o_proj : Linear\n",
            "model.layers.11.mlp : LlamaMLP\n",
            "model.layers.11.mlp.gate_proj : Linear\n",
            "model.layers.11.mlp.up_proj : Linear\n",
            "model.layers.11.mlp.down_proj : Linear\n",
            "model.layers.11.mlp.act_fn : SiLUActivation\n",
            "model.layers.11.input_layernorm : LlamaRMSNorm\n",
            "model.layers.11.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.12 : LlamaDecoderLayer\n",
            "model.layers.12.self_attn : LlamaAttention\n",
            "model.layers.12.self_attn.q_proj : Linear\n",
            "model.layers.12.self_attn.k_proj : Linear\n",
            "model.layers.12.self_attn.v_proj : Linear\n",
            "model.layers.12.self_attn.o_proj : Linear\n",
            "model.layers.12.mlp : LlamaMLP\n",
            "model.layers.12.mlp.gate_proj : Linear\n",
            "model.layers.12.mlp.up_proj : Linear\n",
            "model.layers.12.mlp.down_proj : Linear\n",
            "model.layers.12.mlp.act_fn : SiLUActivation\n",
            "model.layers.12.input_layernorm : LlamaRMSNorm\n",
            "model.layers.12.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.13 : LlamaDecoderLayer\n",
            "model.layers.13.self_attn : LlamaAttention\n",
            "model.layers.13.self_attn.q_proj : Linear\n",
            "model.layers.13.self_attn.k_proj : Linear\n",
            "model.layers.13.self_attn.v_proj : Linear\n",
            "model.layers.13.self_attn.o_proj : Linear\n",
            "model.layers.13.mlp : LlamaMLP\n",
            "model.layers.13.mlp.gate_proj : Linear\n",
            "model.layers.13.mlp.up_proj : Linear\n",
            "model.layers.13.mlp.down_proj : Linear\n",
            "model.layers.13.mlp.act_fn : SiLUActivation\n",
            "model.layers.13.input_layernorm : LlamaRMSNorm\n",
            "model.layers.13.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.14 : LlamaDecoderLayer\n",
            "model.layers.14.self_attn : LlamaAttention\n",
            "model.layers.14.self_attn.q_proj : Linear\n",
            "model.layers.14.self_attn.k_proj : Linear\n",
            "model.layers.14.self_attn.v_proj : Linear\n",
            "model.layers.14.self_attn.o_proj : Linear\n",
            "model.layers.14.mlp : LlamaMLP\n",
            "model.layers.14.mlp.gate_proj : Linear\n",
            "model.layers.14.mlp.up_proj : Linear\n",
            "model.layers.14.mlp.down_proj : Linear\n",
            "model.layers.14.mlp.act_fn : SiLUActivation\n",
            "model.layers.14.input_layernorm : LlamaRMSNorm\n",
            "model.layers.14.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.15 : LlamaDecoderLayer\n",
            "model.layers.15.self_attn : LlamaAttention\n",
            "model.layers.15.self_attn.q_proj : Linear\n",
            "model.layers.15.self_attn.k_proj : Linear\n",
            "model.layers.15.self_attn.v_proj : Linear\n",
            "model.layers.15.self_attn.o_proj : Linear\n",
            "model.layers.15.mlp : LlamaMLP\n",
            "model.layers.15.mlp.gate_proj : Linear\n",
            "model.layers.15.mlp.up_proj : Linear\n",
            "model.layers.15.mlp.down_proj : Linear\n",
            "model.layers.15.mlp.act_fn : SiLUActivation\n",
            "model.layers.15.input_layernorm : LlamaRMSNorm\n",
            "model.layers.15.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.16 : LlamaDecoderLayer\n",
            "model.layers.16.self_attn : LlamaAttention\n",
            "model.layers.16.self_attn.q_proj : Linear\n",
            "model.layers.16.self_attn.k_proj : Linear\n",
            "model.layers.16.self_attn.v_proj : Linear\n",
            "model.layers.16.self_attn.o_proj : Linear\n",
            "model.layers.16.mlp : LlamaMLP\n",
            "model.layers.16.mlp.gate_proj : Linear\n",
            "model.layers.16.mlp.up_proj : Linear\n",
            "model.layers.16.mlp.down_proj : Linear\n",
            "model.layers.16.mlp.act_fn : SiLUActivation\n",
            "model.layers.16.input_layernorm : LlamaRMSNorm\n",
            "model.layers.16.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.17 : LlamaDecoderLayer\n",
            "model.layers.17.self_attn : LlamaAttention\n",
            "model.layers.17.self_attn.q_proj : Linear\n",
            "model.layers.17.self_attn.k_proj : Linear\n",
            "model.layers.17.self_attn.v_proj : Linear\n",
            "model.layers.17.self_attn.o_proj : Linear\n",
            "model.layers.17.mlp : LlamaMLP\n",
            "model.layers.17.mlp.gate_proj : Linear\n",
            "model.layers.17.mlp.up_proj : Linear\n",
            "model.layers.17.mlp.down_proj : Linear\n",
            "model.layers.17.mlp.act_fn : SiLUActivation\n",
            "model.layers.17.input_layernorm : LlamaRMSNorm\n",
            "model.layers.17.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.18 : LlamaDecoderLayer\n",
            "model.layers.18.self_attn : LlamaAttention\n",
            "model.layers.18.self_attn.q_proj : Linear\n",
            "model.layers.18.self_attn.k_proj : Linear\n",
            "model.layers.18.self_attn.v_proj : Linear\n",
            "model.layers.18.self_attn.o_proj : Linear\n",
            "model.layers.18.mlp : LlamaMLP\n",
            "model.layers.18.mlp.gate_proj : Linear\n",
            "model.layers.18.mlp.up_proj : Linear\n",
            "model.layers.18.mlp.down_proj : Linear\n",
            "model.layers.18.mlp.act_fn : SiLUActivation\n",
            "model.layers.18.input_layernorm : LlamaRMSNorm\n",
            "model.layers.18.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.19 : LlamaDecoderLayer\n",
            "model.layers.19.self_attn : LlamaAttention\n",
            "model.layers.19.self_attn.q_proj : Linear\n",
            "model.layers.19.self_attn.k_proj : Linear\n",
            "model.layers.19.self_attn.v_proj : Linear\n",
            "model.layers.19.self_attn.o_proj : Linear\n",
            "model.layers.19.mlp : LlamaMLP\n",
            "model.layers.19.mlp.gate_proj : Linear\n",
            "model.layers.19.mlp.up_proj : Linear\n",
            "model.layers.19.mlp.down_proj : Linear\n",
            "model.layers.19.mlp.act_fn : SiLUActivation\n",
            "model.layers.19.input_layernorm : LlamaRMSNorm\n",
            "model.layers.19.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.20 : LlamaDecoderLayer\n",
            "model.layers.20.self_attn : LlamaAttention\n",
            "model.layers.20.self_attn.q_proj : Linear\n",
            "model.layers.20.self_attn.k_proj : Linear\n",
            "model.layers.20.self_attn.v_proj : Linear\n",
            "model.layers.20.self_attn.o_proj : Linear\n",
            "model.layers.20.mlp : LlamaMLP\n",
            "model.layers.20.mlp.gate_proj : Linear\n",
            "model.layers.20.mlp.up_proj : Linear\n",
            "model.layers.20.mlp.down_proj : Linear\n",
            "model.layers.20.mlp.act_fn : SiLUActivation\n",
            "model.layers.20.input_layernorm : LlamaRMSNorm\n",
            "model.layers.20.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.21 : LlamaDecoderLayer\n",
            "model.layers.21.self_attn : LlamaAttention\n",
            "model.layers.21.self_attn.q_proj : Linear\n",
            "model.layers.21.self_attn.k_proj : Linear\n",
            "model.layers.21.self_attn.v_proj : Linear\n",
            "model.layers.21.self_attn.o_proj : Linear\n",
            "model.layers.21.mlp : LlamaMLP\n",
            "model.layers.21.mlp.gate_proj : Linear\n",
            "model.layers.21.mlp.up_proj : Linear\n",
            "model.layers.21.mlp.down_proj : Linear\n",
            "model.layers.21.mlp.act_fn : SiLUActivation\n",
            "model.layers.21.input_layernorm : LlamaRMSNorm\n",
            "model.layers.21.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.22 : LlamaDecoderLayer\n",
            "model.layers.22.self_attn : LlamaAttention\n",
            "model.layers.22.self_attn.q_proj : Linear\n",
            "model.layers.22.self_attn.k_proj : Linear\n",
            "model.layers.22.self_attn.v_proj : Linear\n",
            "model.layers.22.self_attn.o_proj : Linear\n",
            "model.layers.22.mlp : LlamaMLP\n",
            "model.layers.22.mlp.gate_proj : Linear\n",
            "model.layers.22.mlp.up_proj : Linear\n",
            "model.layers.22.mlp.down_proj : Linear\n",
            "model.layers.22.mlp.act_fn : SiLUActivation\n",
            "model.layers.22.input_layernorm : LlamaRMSNorm\n",
            "model.layers.22.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.23 : LlamaDecoderLayer\n",
            "model.layers.23.self_attn : LlamaAttention\n",
            "model.layers.23.self_attn.q_proj : Linear\n",
            "model.layers.23.self_attn.k_proj : Linear\n",
            "model.layers.23.self_attn.v_proj : Linear\n",
            "model.layers.23.self_attn.o_proj : Linear\n",
            "model.layers.23.mlp : LlamaMLP\n",
            "model.layers.23.mlp.gate_proj : Linear\n",
            "model.layers.23.mlp.up_proj : Linear\n",
            "model.layers.23.mlp.down_proj : Linear\n",
            "model.layers.23.mlp.act_fn : SiLUActivation\n",
            "model.layers.23.input_layernorm : LlamaRMSNorm\n",
            "model.layers.23.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.24 : LlamaDecoderLayer\n",
            "model.layers.24.self_attn : LlamaAttention\n",
            "model.layers.24.self_attn.q_proj : Linear\n",
            "model.layers.24.self_attn.k_proj : Linear\n",
            "model.layers.24.self_attn.v_proj : Linear\n",
            "model.layers.24.self_attn.o_proj : Linear\n",
            "model.layers.24.mlp : LlamaMLP\n",
            "model.layers.24.mlp.gate_proj : Linear\n",
            "model.layers.24.mlp.up_proj : Linear\n",
            "model.layers.24.mlp.down_proj : Linear\n",
            "model.layers.24.mlp.act_fn : SiLUActivation\n",
            "model.layers.24.input_layernorm : LlamaRMSNorm\n",
            "model.layers.24.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.25 : LlamaDecoderLayer\n",
            "model.layers.25.self_attn : LlamaAttention\n",
            "model.layers.25.self_attn.q_proj : Linear\n",
            "model.layers.25.self_attn.k_proj : Linear\n",
            "model.layers.25.self_attn.v_proj : Linear\n",
            "model.layers.25.self_attn.o_proj : Linear\n",
            "model.layers.25.mlp : LlamaMLP\n",
            "model.layers.25.mlp.gate_proj : Linear\n",
            "model.layers.25.mlp.up_proj : Linear\n",
            "model.layers.25.mlp.down_proj : Linear\n",
            "model.layers.25.mlp.act_fn : SiLUActivation\n",
            "model.layers.25.input_layernorm : LlamaRMSNorm\n",
            "model.layers.25.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.26 : LlamaDecoderLayer\n",
            "model.layers.26.self_attn : LlamaAttention\n",
            "model.layers.26.self_attn.q_proj : Linear\n",
            "model.layers.26.self_attn.k_proj : Linear\n",
            "model.layers.26.self_attn.v_proj : Linear\n",
            "model.layers.26.self_attn.o_proj : Linear\n",
            "model.layers.26.mlp : LlamaMLP\n",
            "model.layers.26.mlp.gate_proj : Linear\n",
            "model.layers.26.mlp.up_proj : Linear\n",
            "model.layers.26.mlp.down_proj : Linear\n",
            "model.layers.26.mlp.act_fn : SiLUActivation\n",
            "model.layers.26.input_layernorm : LlamaRMSNorm\n",
            "model.layers.26.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.27 : LlamaDecoderLayer\n",
            "model.layers.27.self_attn : LlamaAttention\n",
            "model.layers.27.self_attn.q_proj : Linear\n",
            "model.layers.27.self_attn.k_proj : Linear\n",
            "model.layers.27.self_attn.v_proj : Linear\n",
            "model.layers.27.self_attn.o_proj : Linear\n",
            "model.layers.27.mlp : LlamaMLP\n",
            "model.layers.27.mlp.gate_proj : Linear\n",
            "model.layers.27.mlp.up_proj : Linear\n",
            "model.layers.27.mlp.down_proj : Linear\n",
            "model.layers.27.mlp.act_fn : SiLUActivation\n",
            "model.layers.27.input_layernorm : LlamaRMSNorm\n",
            "model.layers.27.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.28 : LlamaDecoderLayer\n",
            "model.layers.28.self_attn : LlamaAttention\n",
            "model.layers.28.self_attn.q_proj : Linear\n",
            "model.layers.28.self_attn.k_proj : Linear\n",
            "model.layers.28.self_attn.v_proj : Linear\n",
            "model.layers.28.self_attn.o_proj : Linear\n",
            "model.layers.28.mlp : LlamaMLP\n",
            "model.layers.28.mlp.gate_proj : Linear\n",
            "model.layers.28.mlp.up_proj : Linear\n",
            "model.layers.28.mlp.down_proj : Linear\n",
            "model.layers.28.mlp.act_fn : SiLUActivation\n",
            "model.layers.28.input_layernorm : LlamaRMSNorm\n",
            "model.layers.28.post_attention_layernorm : LlamaRMSNorm\n",
            "model.layers.29 : LlamaDecoderLayer\n",
            "model.layers.29.self_attn : LlamaAttention\n",
            "model.layers.29.self_attn.q_proj : Linear\n",
            "model.layers.29.self_attn.k_proj : Linear\n",
            "model.layers.29.self_attn.v_proj : Linear\n",
            "model.layers.29.self_attn.o_proj : Linear\n",
            "model.layers.29.mlp : LlamaMLP\n",
            "model.layers.29.mlp.gate_proj : Linear\n",
            "model.layers.29.mlp.up_proj : Linear\n",
            "model.layers.29.mlp.down_proj : Linear\n",
            "model.layers.29.mlp.act_fn : SiLUActivation\n",
            "model.layers.29.input_layernorm : LlamaRMSNorm\n",
            "model.layers.29.post_attention_layernorm : LlamaRMSNorm\n",
            "model.norm : LlamaRMSNorm\n",
            "model.rotary_emb : LlamaRotaryEmbedding\n",
            "lm_head : Linear\n",
            "\n",
            "Done. You now have:\n",
            " - All config\n",
            " - Tokenizer details\n",
            " - Weight key â†’ shape mapping\n",
            " - First 5 layer structure\n",
            " - Full transformers architecture tree\n",
            "we can use these to write an identical model.py implementation.\n"
          ]
        }
      ],
      "source": [
        "# 4. Load HF model and print its architecture summary\n",
        "print(\"\\n======================== TRANSFORMERS MODEL SUMMARY ========================\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    REPO,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"cpu\"\n",
        ")\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Print submodules with depth\n",
        "print(\"\\n======================== MODULE TREE ========================\")\n",
        "for name, module in model.named_modules():\n",
        "    print(name, \":\", module.__class__.__name__)\n",
        "\n",
        "print(\"\\nDone. You now have:\")\n",
        "print(\" - All config\")\n",
        "print(\" - Tokenizer details\")\n",
        "print(\" - Weight key â†’ shape mapping\")\n",
        "print(\" - First 5 layer structure\")\n",
        "print(\" - Full transformers architecture tree\")\n",
        "print(\"we can use these to write an identical model.py implementation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbXCvDITmQT1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From above all the info, here is the analysis:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ”¹Architectural Hyperparameters\n",
        "===============================\n",
        "hidden_size = 576\n",
        "num_hidden_layers = 30\n",
        "num_attention_heads = 9\n",
        "num_key_value_heads = 3\n",
        "intermediate_size = 1536\n",
        "vocab_size = 49152\n",
        "rms_norm_eps = 1e-6\n",
        "activation = silu\n",
        "rope_theta\n",
        "rope_scaling\n",
        "rope_interleaved\n",
        "\n",
        "\n",
        "ðŸ”¹Weight Shapes\n",
        "=============\n",
        "model.layers.0.self_attn.q_proj.weight  (576, 576)\n",
        "model.layers.0.self_attn.k_proj.weight  (576, 192)\n",
        "model.layers.0.self_attn.v_proj.weight  (576, 192)\n",
        "model.layers.0.self_attn.o_proj.weight  (576, 576)\n",
        "model.layers.0.mlp.gate_proj.weight     (1536, 576)\n",
        "\n",
        "These shapes tell you:\n",
        "head_dim = 576 / 9 = 64\n",
        "kv_dim = 3 Ã— 64 = 192\n",
        "MLP dimension matches config (1536)\n",
        "RoPE used instead of positional embeddings\n",
        "RMSNorm everywhere\n",
        "\n",
        "ðŸ”¹Layer structure\n",
        "=================\n",
        "model.embed_tokens\n",
        "model.layers.0.attn_norm\n",
        "model.layers.0.attn\n",
        "model.layers.0.ffn_norm\n",
        "model.layers.0.mlp\n",
        "model.norm\n",
        "model.lm_head"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
